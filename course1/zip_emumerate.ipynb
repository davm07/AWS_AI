{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Zip Model Metrics\n",
    "\n",
    "Use `zip` to write a for loop that creates a string specifying the model name and its corresponding metrics (accuracy, precision, recall) and appends it to the list `model_metrics`.\n",
    "\n",
    "Each string should be formatted as `model: accuracy, precision, recall`. For example, the string for the first model should be `Model1: 0.95, 0.94, 0.93`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\"]\n",
    "accuracy = [0.95, 0.89, 0.92, 0.87, 0.93]\n",
    "precision = [0.94, 0.88, 0.91, 0.86, 0.92]\n",
    "recall = [0.93, 0.87, 0.91, 0.85, 0.91]\n",
    "model_metrics = []\n",
    "\n",
    "# write your for loop here\n",
    "# TODO\n",
    "for model, accu, prec, rec in zip(model_names, accuracy, precision, recall):\n",
    "    metrics = f\"{model}: {accu:.2f}, {prec:.2f}, {rec:.2f}\"\n",
    "    model_metrics.append(metrics)\n",
    "\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = [\"Model1: 0.95, 0.94, 0.93\", \"Model2: 0.89, 0.88, 0.87\", \"Model3: 0.92, 0.91, 0.91\", \"Model4: 0.87, 0.86, 0.85\", \"Model5: 0.93, 0.92, 0.91\"]\n",
    "if model_metrics == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Zip Metrics to Dictionary\n",
    "\n",
    "Use `zip` to create a dictionary `model_performance` that uses `model_names` as keys and `accuracies` as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\"]\n",
    "accuracies = [0.95, 0.89, 0.92, 0.87, 0.93]\n",
    "\n",
    "model_performance = {}\n",
    "for model, accu in zip(model_names, accuracies):\n",
    "    model_performance[model] = accu\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = {\"Model1\": 0.95, \"Model2\": 0.89, \"Model3\": 0.92, \"Model4\": 0.87, \"Model5\": 0.93}\n",
    "if model_performance == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Unzip Model Metrics\n",
    "\n",
    "Unzip the `model_performance` tuple into two `model_names` and `accuracies` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_performance = ((\"Model1\", 0.95), (\"Model2\", 0.89), (\"Model3\", 0.92), (\"Model4\", 0.87), (\"Model5\", 0.93))\n",
    "\n",
    "# define model_names and accuracies here\n",
    "model_names, accuracies = zip(*model_performance)\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer_names = (\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\")\n",
    "correct_answer_accuracies = (0.95, 0.89, 0.92, 0.87, 0.93)\n",
    "if model_names == correct_answer_names and accuracies == correct_answer_accuracies:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Transpose Metrics Matrix\n",
    "\n",
    "Use `zip` to transpose `metrics_data` from a 4-by-3 matrix to a 3-by-4 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "metrics_data = ((0.95, 0.94, 0.93), (0.89, 0.88, 0.87), (0.92, 0.91, 0.90), (0.87, 0.86, 0.85))\n",
    "\n",
    "# Transpose the matrix\n",
    "metrics_data_transpose = list(zip(*metrics_data))\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = [(0.95, 0.89, 0.92, 0.87), (0.94, 0.88, 0.91, 0.86), (0.93, 0.87, 0.90, 0.85)]\n",
    "if metrics_data_transpose == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Enumerate Model Performances\n",
    "\n",
    "Use `enumerate` to modify the `model_descriptions` list so that each element contains the model name followed by its corresponding accuracy. For example, the first element of `model_descriptions` should change from `\"Model1 Description\"` to `\"Model1 Description 0.95\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_descriptions = [\"Model1 Description\", \"Model2 Description\", \"Model3 Description\", \"Model4 Description\", \"Model5 Description\"]\n",
    "accuracies = [0.95, 0.89, 0.92, 0.87, 0.93]\n",
    "\n",
    "# write your for loop here\n",
    "for i, description in enumerate(model_descriptions):\n",
    "    model_descriptions[i] = f\"{description} {accuracies[i]:.2f}\"\n",
    "\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = [\"Model1 Description 0.95\", \"Model2 Description 0.89\", \"Model3 Description 0.92\", \"Model4 Description 0.87\", \"Model5 Description 0.93\"]\n",
    "if model_descriptions == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  }
 ],
 "metadata": {
  "grader_mode": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "showGradeBtn": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
