{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Extract Model Acronyms\n",
    "\n",
    "In this exercise, you'll use a list comprehension to create a new list containing just the acronyms of model names. Each acronym is made by taking the first letter of each word in the model name, converting it to uppercase, and joining the letters together.\n",
    "\n",
    "**Example Input**:\n",
    "```python\n",
    "models = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"Support Vector Machine\", \"Naive Bayes\"]\n",
    "```\n",
    "\n",
    "**Example Output**:\n",
    "```python\n",
    "model_acronyms = ['LR', 'DT', 'RF', 'SVM', 'NB']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "models = [\"Logistic Regression\", \"Decision Tree\", \"Random Forest\", \"Support Vector Machine\", \"Naive Bayes\"]\n",
    "model_acronyms = [''.join([word[0].upper() for word in model.split()]) for model in models]\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = ['LR', 'DT', 'RF', 'SVM', 'NB']\n",
    "if model_acronyms == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Not quite! Did you create the acronyms correctly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Learning Rate Decay\n",
    "\n",
    "In this exercise, you'll use a list comprehension to create a list containing the learning rates (LR) a model will see during training, with an initial learning rate of 0.1 and a decay factor of 0.1. The decay is applied multiplicatively for each subsequent step. Generate the learning rates for the first 5 steps.\n",
    "\n",
    "**Example Output**:\n",
    "```python\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.100000', '0.010000', '0.001000', '0.000100', '0.000010', '0.000001']\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "initial_lr = 0.1\n",
    "decay_factor = 0.1\n",
    "learning_rates = [initial_lr * (decay_factor**i) for i in range(6)]\n",
    "learning_rates = [format(lr, '.6f') for lr in learning_rates]\n",
    "print(learning_rates)\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = ['0.100000', '0.010000', '0.001000', '0.000100', '0.000010', '0.000001']\n",
    "if learning_rates == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Not quite! Are you sure the decay factor is applied correctly for each step?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Filter Models by Performance\n",
    "\n",
    "In this exercise, you'll use a list comprehension to create a list of model names that achieved at least 85% performance.\n",
    "\n",
    "**Example Input**:\n",
    "```python\n",
    "model_performances = {\n",
    "    \"Logistic Regression\": 90,\n",
    "    \"Decision Tree\": 75,\n",
    "    \"Random Forest\": 92,\n",
    "    \"Support Vector Machine\": 80,\n",
    "    \"Naive Bayes\": 88\n",
    "}\n",
    "```\n",
    "\n",
    "**Example Output**:\n",
    "```python\n",
    "passed_models = ['Logistic Regression', 'Random Forest', 'Naive Bayes']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_performances = {\n",
    "    \"Logistic Regression\": 90,\n",
    "    \"Decision Tree\": 75,\n",
    "    \"Random Forest\": 92,\n",
    "    \"Support Vector Machine\": 80,\n",
    "    \"Naive Bayes\": 88\n",
    "}\n",
    "\n",
    "passed_models = [model for model, performance in model_performances.items() if performance >= 85]\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = ['Logistic Regression', 'Random Forest', 'Naive Bayes']\n",
    "if passed_models == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Not quite! Did you use the corresponding performance to filter each model?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
